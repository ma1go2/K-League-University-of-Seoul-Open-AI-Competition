{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576ed9f8-83d0-4732-acd2-912b265d317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 준비\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import copy\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc45b073-8320-424c-aba6-10ab965df400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas==2.3.3\n",
      "numpy==2.4.1\n",
      "torch==2.10.0+cu128\n",
      "sklearn not installed\n",
      "os not installed\n",
      "copy not installed\n",
      "warnings not installed\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata as metadata  # Python 3.8+\n",
    "\n",
    "libs = [\"pandas\", \"numpy\", \"torch\", \"sklearn\", \"os\", \"copy\", \"warnings\"]\n",
    "\n",
    "for name in libs:\n",
    "    try:\n",
    "        print(name, '==',metadata.version(name), sep='')\n",
    "    except metadata.PackageNotFoundError:\n",
    "        print(name, \"not installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a59e53-38a1-4bb1-a0d0-5694cec6fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14f8fe1faf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 0. 설정 및 파라미터\n",
    "# ==========================================\n",
    "PITCH_X = 105.0\n",
    "PITCH_Y = 68.\n",
    "\n",
    "BATCH_SIZE = 64    # 샘플 수\n",
    "EPOCHS = 35        # 반복 학습 횟수\n",
    "PATIENCE = 7       # 검증 손실(Val Loss)이 7번(Epoch) 연속으로 좋아지지 않으면 학습 중단 (Early Stopping)\n",
    "N_SPLITS = 5       # 교차 검증(Cross-Validation)의 그룹 수\n",
    "MAX_SEQ_LEN = 50   # 과거 이벤트 최대 50개 까지 봄\n",
    "\n",
    "# 모델 파라미터\n",
    "HIDDEN_DIM = 128   # LSTM 내부 뉴런 개수 (클수록 모델의 표현력(기억 용량)이 좋아지지만, 과적합 위험이 커짐)\n",
    "NUM_LAYERS = 2     # LSTM 층을 2겹으로 쌓음 => 복잡한 시계열 패턴을 학습할 수 있음\n",
    "CONTEXT_LEN = 10   # 마지막 10개의 이벤트에 대해 Attention 메커니즘을 적용하여 가중치 계산\n",
    "DROPOUT = 0.3      # 뉴런의 30%를 무작위로 끊어 과적합 방지\n",
    "\n",
    "# 학습 파라미터\n",
    "LR = 0.001         # 학습률(Learning Rate). Adam 옵티마이저 가중치를 이동시키는 보폭\n",
    "ALPHA = 0.5        # Huber Loss + Euclidean Distance 결합 손실 함수용 파라미터\n",
    "DELTA = 0.05       # - 이상치에 덜 민감하면서도, 실제 거리 오차를 줄이는 균형 잡힌 설정\n",
    "LAMBDA_POS = 0.3   # 전체 손실(Total Loss) = 이동벡터 오차 + (Lamda_POS * 절대좌표 오차)\n",
    "                   # - 이동 벡터를 맞추는 것이 목표이지만, 도착 위치(절대 좌표)도 맞추도록 학습 시킴\n",
    "                   # - 적절히 모델의 공간 지각 능력을 돕도록 30% 정도의 비중만 둠\n",
    "\n",
    "# 피처 리스트 (venue_id 추가됨)\n",
    "FEAT_COLS = [\n",
    "    'start_x_n', 'start_y_n',\n",
    "    'type_id', 'result_id',\n",
    "    'is_home', 'team_id_encoded',\n",
    "    'distance', 'angle_norm',\n",
    "    'velocity_norm', 'zone_normalized',\n",
    "    'distance_bin_norm', 'relative_angle_norm',\n",
    "    'prev_end_x_n', 'prev_end_y_n',\n",
    "    'prev_dx_n', 'prev_dy_n',\n",
    "    'prev_velocity_norm', 'prev_distance_bin_norm',\n",
    "    'time_delta_n', 'prev_type_id',\n",
    "    'venue_id',   \n",
    "    'month_sin',  \n",
    "    'month_cos'   \n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "np.random.seed(2025)\n",
    "torch.manual_seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21468543-c7a7-4e4e-89df-54e6055d2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. 데이터 로드 및 전처리 (Match Info & Augmentation)\n",
    "# ==========================================\n",
    "\n",
    "def load_match_info(path='match_info.csv'):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {path} not found. Skipping match info.\")\n",
    "        return None, {}\n",
    "        \n",
    "    df_match = pd.read_csv(path)\n",
    "    \n",
    "    # 1. Venue Encoding\n",
    "    unique_venues = df_match['venue'].unique()\n",
    "    venue2id = {v: i+1 for i, v in enumerate(unique_venues)}\n",
    "    df_match['venue_id'] = df_match['venue'].map(venue2id).fillna(0).astype(int)\n",
    "    \n",
    "    # 2. 날짜 파싱 및 월(Month) 추출\n",
    "    # 'game_date'를 datetime 객체로 변환\n",
    "    df_match['game_date'] = pd.to_datetime(df_match['game_date'])\n",
    "    \n",
    "    # 월(Month) 추출 (1~12)\n",
    "    df_match['month'] = df_match['game_date'].dt.month\n",
    "    \n",
    "    # 3. Cyclical Encoding (월 정보를 연속적인 원형 데이터로 변환)\n",
    "    # 12월과 1월이 이어지도록 sin, cos 변환\n",
    "    # month - 1을 하는 이유: 0~11로 맞춰주기 위함\n",
    "    df_match['month_sin'] = np.sin(2 * np.pi * (df_match['month'] - 1) / 12)\n",
    "    df_match['month_cos'] = np.cos(2 * np.pi * (df_match['month'] - 1) / 12)\n",
    "    \n",
    "    # 메타 정보 저장\n",
    "    meta_match = {'venue2id': venue2id}\n",
    "    \n",
    "    # 필요한 컬럼 선택 (month_sin, month_cos 추가)\n",
    "    # game_id를 기준으로 병합하므로 반드시 포함\n",
    "    df_match_filtered = df_match[['game_id', 'venue_id', 'month_sin', 'month_cos']]\n",
    "    \n",
    "    return df_match_filtered, meta_match\n",
    "\n",
    "def augment_data(df):\n",
    "    print(\"데이터 증강 수행 중 (Flip Y)...\")\n",
    "    df_aug = df.copy()\n",
    "    \n",
    "    # 1. Y 좌표 반전\n",
    "    df_aug['start_y'] = PITCH_Y - df_aug['start_y']\n",
    "    df_aug['end_y'] = PITCH_Y - df_aug['end_y']\n",
    "    \n",
    "    # 2. 에피소드 ID 변경 (중복 방지)\n",
    "    # game_id는 유지하여 GroupKFold에서 같은 폴드에 들어가게 함 (Data Leakage 방지)\n",
    "    df_aug['game_episode'] = df_aug['game_episode'].astype(str) + '_aug'\n",
    "    \n",
    "    # 3. 합치기\n",
    "    df_final = pd.concat([df, df_aug], axis=0).reset_index(drop=True)\n",
    "    return df_final\n",
    "\n",
    "def load_raw_and_process(train_path, match_path='match_info.csv', is_train=True):\n",
    "    # 1. 메인 데이터 로드\n",
    "    df = pd.read_csv(train_path)\n",
    "    \n",
    "    # 2. Match Info 병합\n",
    "    df_match, match_meta = load_match_info(match_path)\n",
    "    if df_match is not None:\n",
    "        df = pd.merge(df, df_match, on='game_id', how='left')\n",
    "        df['venue_id'] = df['venue_id'].fillna(0).astype(int)\n",
    "    else:\n",
    "        df['venue_id'] = 0\n",
    "        match_meta = {}\n",
    "\n",
    "    # 3. 정렬\n",
    "    df = df.sort_values(['game_id', 'period_id', 'episode_id', 'time_seconds', 'action_id'])\n",
    "    \n",
    "    # 4. [Train Only] 데이터 증강\n",
    "    if is_train:\n",
    "        df = augment_data(df)\n",
    "        \n",
    "    df = df.reset_index(drop=True)\n",
    "    return df, match_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd531f67-ac08-48f5-8479-e1af8a945a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. 피처 엔지니어링 (기존 로직 + Venue)\n",
    "# ==========================================\n",
    "def make_event_features(df: pd.DataFrame, meta: dict = None, is_train: bool = True) -> tuple:\n",
    "    df_feat = df.copy()\n",
    "    \n",
    "    # 0. 카테고리 인코딩\n",
    "    if is_train:\n",
    "        type_cols = df_feat['type_name'].unique()\n",
    "        type2id = {t: i+1 for i, t in enumerate(type_cols)}\n",
    "        \n",
    "        unique_results = df_feat['result_name'].fillna('Unknown').unique()\n",
    "        if 'Unknown' not in unique_results:\n",
    "            unique_results = np.append(unique_results, 'Unknown')\n",
    "        res2id = {r: i+1 for i, r in enumerate(unique_results)}\n",
    "        \n",
    "        unique_teams = df_feat['team_id'].dropna().unique()\n",
    "        team2id = {team: i+1 for i, team in enumerate(unique_teams)}\n",
    "        \n",
    "        # match_meta 병합\n",
    "        if 'venue2id' in meta:\n",
    "            venue2id = meta['venue2id']\n",
    "        else:\n",
    "             venue2id = {} # Fallback\n",
    "\n",
    "        meta.update({\n",
    "            'type2id': type2id,\n",
    "            'res2id': res2id,\n",
    "            'team2id': team2id,\n",
    "        })\n",
    "    else:\n",
    "        type2id = meta['type2id']\n",
    "        res2id = meta['res2id']\n",
    "        team2id = meta['team2id']\n",
    "    # 매핑 적용\n",
    "    df_feat['type_id'] = df_feat['type_name'].map(type2id).fillna(0).astype(int)\n",
    "    df_feat['result_id'] = df_feat['result_name'].fillna('Unknown').map(res2id).fillna(0).astype(int)\n",
    "    df_feat['team_id_encoded'] = df_feat['team_id'].map(team2id).fillna(0).astype(int)\n",
    "    \n",
    "    # 1. 기본 이동 벡터 계산\n",
    "    df_feat['dx'] = df_feat['end_x'] - df_feat['start_x']\n",
    "    df_feat['dy'] = df_feat['end_y'] - df_feat['start_y']\n",
    "    df_feat['distance'] = np.sqrt(df_feat['dx']**2 + df_feat['dy']**2)\n",
    "    \n",
    "    # 2. 좌표 정규화 (0~1)\n",
    "    df_feat['start_x_n'] = df_feat['start_x'] / PITCH_X\n",
    "    df_feat['start_y_n'] = df_feat['start_y'] / PITCH_Y\n",
    "    df_feat['end_x_n'] = df_feat['end_x'] / PITCH_X\n",
    "    df_feat['end_y_n'] = df_feat['end_y'] / PITCH_Y\n",
    "    \n",
    "    # 3. 패스 각도\n",
    "    df_feat['angle_rad'] = np.arctan2(df_feat['dy'], df_feat['dx'])\n",
    "    df_feat['angle_deg'] = np.degrees(df_feat['angle_rad'])\n",
    "    df_feat['angle_norm'] = (df_feat['angle_deg'] + 180) / 360.0\n",
    "    \n",
    "    # 4. 시간 정보 (GroupBy 기반)\n",
    "    g = df_feat.groupby('game_episode')\n",
    "    df_feat['time_delta'] = g['time_seconds'].diff().fillna(0)\n",
    "    df_feat['time_delta_n'] = np.clip(df_feat['time_delta'] / 7.0, 0, 1)\n",
    "    \n",
    "    # 5. 속도 계산\n",
    "    df_feat['velocity'] = np.where(\n",
    "        df_feat['time_delta'] > 0.1,\n",
    "        df_feat['distance'] / df_feat['time_delta'],\n",
    "        0\n",
    "    )\n",
    "    df_feat['velocity_norm'] = np.clip(df_feat['velocity'] / 17.0, 0, 1)\n",
    "    \n",
    "    # 6. 구역 피처 (3x3 그리드)\n",
    "    df_feat['x_zone'] = np.clip((df_feat['start_x_n'] * 3).astype(int), 0, 2)\n",
    "    df_feat['y_zone'] = np.clip((df_feat['start_y_n'] * 3).astype(int), 0, 2)\n",
    "    df_feat['zone_id'] = df_feat['x_zone'] * 3 + df_feat['y_zone']\n",
    "    df_feat['zone_normalized'] = df_feat['zone_id'] / 9.0\n",
    "    \n",
    "    # 7. 거리 구간\n",
    "    def get_distance_bin(dist):\n",
    "        if dist < 5:\n",
    "            return 0\n",
    "        elif dist < 15:\n",
    "            return 1\n",
    "        elif dist < 30:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    df_feat['distance_bin'] = df_feat['distance'].apply(get_distance_bin)\n",
    "    df_feat['distance_bin_norm'] = df_feat['distance_bin'] / 3.0\n",
    "    \n",
    "    # 8. 상대 각도\n",
    "    df_feat['prev_angle_deg'] = g['angle_deg'].shift(1).fillna(0)\n",
    "    angle_diff = df_feat['angle_deg'] - df_feat['prev_angle_deg']\n",
    "    angle_diff = np.where(angle_diff > 180, angle_diff - 360, angle_diff)\n",
    "    angle_diff = np.where(angle_diff < -180, angle_diff + 360, angle_diff)\n",
    "    df_feat['relative_angle_norm'] = (angle_diff + 180) / 360.0\n",
    "    \n",
    "    # 9. is_home 타입 변환\n",
    "    df_feat['is_home'] = df_feat['is_home'].astype(int)\n",
    "\n",
    "    # 10. Lag Features (직전 정보)\n",
    "    df_feat['prev_end_x_n'] = g['end_x_n'].shift(1).fillna(0)\n",
    "    df_feat['prev_end_y_n'] = g['end_y_n'].shift(1).fillna(0)\n",
    "    df_feat['prev_dx_n'] = (g['dx'].shift(1) / PITCH_X).fillna(0)\n",
    "    df_feat['prev_dy_n'] = (g['dy'].shift(1) / PITCH_Y).fillna(0)\n",
    "    df_feat['prev_velocity_norm'] = g['velocity_norm'].shift(1).fillna(0)\n",
    "    df_feat['prev_distance_bin_norm'] = g['distance_bin_norm'].shift(1).fillna(0)\n",
    "    df_feat['prev_type_id'] = g['type_id'].shift(1).fillna(0)\n",
    "    \n",
    "    # 11. NaN/Inf 처리\n",
    "    df_feat = df_feat.fillna(0)\n",
    "    df_feat = df_feat.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return df_feat, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c37e4b6-13cf-47fe-a4b5-42b8cc42419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. 시퀀스 구축 (Target에 절대좌표 추가)\n",
    "# ==========================================\n",
    "\n",
    "def build_episode_sequences(df, meta, max_len=50, min_pass_in_episode=1, feat_cols=FEAT_COLS):\n",
    "    X_list, y_list, info_list = [], [], []\n",
    "    \n",
    "    for ep, g in df.groupby('game_episode'):    # 에피소드 그룹화\n",
    "        pass_rows = g[g['type_name'] == 'Pass']\n",
    "        if len(pass_rows) < min_pass_in_episode: continue\n",
    "        \n",
    "        last_pass_idx = pass_rows.index[-1]\n",
    "        last_pass = df.loc[last_pass_idx]\n",
    "\n",
    "        # 절대 좌표와 변화량 모두 계산\n",
    "        start_x_n, start_y_n = last_pass['start_x_n'], last_pass['start_y_n']\n",
    "        end_x_n, end_y_n = last_pass['end_x_n'], last_pass['end_y_n'] # 절대좌표 타겟\n",
    "        target_dx, target_dy = end_x_n - start_x_n, end_y_n - start_y_n # 변화량\n",
    "\n",
    "        # 입력 시퀀스 구축\n",
    "        g_input = g[g.index < last_pass_idx].copy()\n",
    "        if len(g_input) == 0: continue\n",
    "        \n",
    "        seq = g_input[feat_cols].copy()\n",
    "        seq_vals = seq.to_numpy(dtype=float)\n",
    "        \n",
    "        seq_len = len(seq_vals)\n",
    "\n",
    "        # 길이 고정\n",
    "        if seq_len > max_len:\n",
    "            seq_vals = seq_vals[-max_len:]\n",
    "        elif seq_len < max_len:\n",
    "            pad = np.zeros((max_len - seq_len, seq_vals.shape[1]), dtype=float) # Zero Padding: 부족한 만큼 0으로 채움\n",
    "            seq_vals = np.concatenate([pad, seq_vals], axis=0)                  # - 앞에 넣는 이유: LSTM 시퀀스의 최근 정보를 더 잘 기억하기 때문에\n",
    "        \n",
    "        X_list.append(seq_vals)\n",
    "        # Target: [dx, dy, end_x, end_y] -> Multi-task learning\n",
    "        y_list.append([target_dx, target_dy, end_x_n, end_y_n]) \n",
    "\n",
    "        # 메타 정보 저장\n",
    "        info_list.append({\n",
    "            'game_episode': ep,\n",
    "            'game_id': g['game_id'].iloc[0], # game_id는 원본 유지 (split용)\n",
    "            'last_start_x_n': start_x_n,\n",
    "            'last_start_y_n': start_y_n,\n",
    "        })\n",
    "    \n",
    "    X = np.stack(X_list, axis=0) if X_list else np.array([])\n",
    "    y = np.array(y_list, dtype=float) if y_list else np.array([])\n",
    "    info_df = pd.DataFrame(info_list)\n",
    "    return X, y, info_df, feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "406bcc3a-d9be-422d-9850-a320558ba9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3.5. 데이터셋 클래스 정의\n",
    "# ==========================================\n",
    "\n",
    "class PassTraceDataset(Dataset):\n",
    "    # 1. 초기화\n",
    "    def __init__(self, X, y=None):\n",
    "        # NumPy 배열을 PyTorch의 실수형 텐서(FloatTensor)로 변환\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        # y(정답)가 있으면 변환하고, 없으면(테스트용) None으로 둠\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "    \n",
    "    # 2. 길이 반환\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # 3. 데이터 꺼내기\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]    # 학습용\n",
    "        return self.X[idx]    # 테스트용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa8672-de2a-4f40-9d98-fc2b54f83338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. 모델 정의(Attention + Multi-task)\n",
    "# ==========================================\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, 1) # Bi-LSTM(양방향)이라 정보량이 *2배\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # 1. 점수 계산 (Scoring)\n",
    "        # - 각 시점(t)마다 점수를 계산\n",
    "        scores = self.attn(lstm_output)\n",
    "\n",
    "        # 2. 가중치 변환 (Softmax)\n",
    "        # - 점수들을 확률처럼 변환(전체 합 1)\n",
    "        attn_weights = F.softmax(scores, dim=1) \n",
    "\n",
    "        # 3. 컨텍스트 벡터 생성 (Weighted Sum)\n",
    "        # - 각 시점의 LSTM 출력값에 위에서 구한 가중치를 곱해서 더함\n",
    "        # - 즉, 중요한 시점의 정보는 많이 반영, 안 중요한 정보는 무시\n",
    "        context = torch.sum(attn_weights * lstm_output, dim=1) \n",
    "        return context, attn_weights\n",
    "        \n",
    "class SoccerLSTM_Advanced(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, dropout=0.3, context_len=10):\n",
    "        super(SoccerLSTM_Advanced, self).__init__()\n",
    "\n",
    "        # 1. Bi-LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,    # 과거, 미래 쌍방향 읽음 => 출력크기 hidden_dim * 2\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # 2. 집중 범위\n",
    "        self.context_len = context_len    # 최근 10개 이벤트에 집중\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # 3. 멀티태스크 헤드\n",
    "        # Head 1: 이동 벡터 예측 (dx, dy)\n",
    "        self.fc_delta = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        # Head 2: 절대 좌표 예측 (x, y) - Auxiliary Task\n",
    "        self.fc_pos = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Sigmoid() # 좌표는 0~1 사이 정규화되어 있음\n",
    "        )\n",
    "    \n",
    "    # 순전파\n",
    "    def forward(self, x):\n",
    "        # 1. 흐름 읽기 (LSTM)\n",
    "        # lstm_out: 전체 시퀀스의 특징을 추출한 데이터\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # 2. 최근 상황 잘라내기\n",
    "        # - 전체 50개 중 마지막 10(context_len)개만 잘라냄\n",
    "        ctx_seq = lstm_out[:, -self.context_len:, :]\n",
    "\n",
    "        # 3. 중요도 판단(Attention)\n",
    "        # 10개 중에서 특히 중요한 순간을 골라 요약 정보(context)로 만듦\n",
    "        context, _ = self.attention(ctx_seq)\n",
    "\n",
    "        # 4. 두 가지 예측 수행 (Multi-task)\n",
    "        # - 요약 정보(context)를 바탕으로 두 가지 질문에 답함\n",
    "        pred_delta = self.fc_delta(context) # Q1. 이동 벡터는? (dx, dy)\n",
    "        pred_pos = self.fc_pos(context)     # Q2. 절대 위치는? (x, y)\n",
    "        \n",
    "        return pred_delta, pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "325b0802-387c-4db6-800d-5b513e8757d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. 손실 함수 (Combined Multi-task Loss)\n",
    "# ==========================================\n",
    "class CombinedMultiTaskLoss(nn.Module):\n",
    "    # 1. 채점 도구 준비\n",
    "    def __init__(self, delta=0.05, alpha=0.5, lambda_pos=0.3):\n",
    "        super(CombinedMultiTaskLoss, self).__init__()\n",
    "        self.delta = delta            # Huber Loss의 기준점 (작은 오차 vs 큰 오차 구분선)\n",
    "        self.alpha = alpha            # Huber Loss와 Euclidean Distance 사이의 비중\n",
    "        self.lambda_pos = lambda_pos  # 메인 문제(이동량)와 보조 문제(위치) 사이의 비중\n",
    "        self.mse = nn.MSELoss()       # 일반적인 제곱 오차 계산기\n",
    "\n",
    "    # 2. 예측 패스의 방향과 거리(delta)가 얼마나 틀렸는지 계산\n",
    "    def huber_euclid(self, pred, target):\n",
    "        # 2.1. Huber Loss (휴버 손실)\n",
    "        # - 이상치에 휘둘리지 않게, 작은 오차에는 정교함을 높이고, 큰 오차에는 적당히 넘어감\n",
    "        diff = pred - target\n",
    "        abs_diff = torch.abs(diff)\n",
    "        huber = torch.where(\n",
    "            abs_diff < self.delta,        \n",
    "            0.5 * diff**2 / self.delta,   # 오차가 아주 작으면 (0.05 미만) 제곱해서 부드럽게 (MSE 처럼)\n",
    "            abs_diff - 0.5 * self.delta   # 오차가 크면 절대값으로 (MAE 처럼)\n",
    "        ).sum(dim=1)\n",
    "        # 2.2. Euclidean Distance\n",
    "        # - 실제 물리적 거리를 줄임\n",
    "        euclid = torch.sqrt(torch.sum(diff**2, dim=1) + 1e-8)     # 0으로 나누는 오류 방지\n",
    "        # 2.3. 결합 (alpha)\n",
    "        # - Huber, Euclidean 50:50 으로 섞어서 점수를 냄\n",
    "        return self.alpha * huber.mean() + (1 - self.alpha) * euclid.mean()\n",
    "\n",
    "    # 3. 전체 점수 (Multi-task)\n",
    "    def forward(self, pred_delta, target_delta, pred_pos, target_pos):\n",
    "        # 1. Delta Loss (Main): 패스 변화량(dx, dy) 채점\n",
    "        loss_delta = self.huber_euclid(pred_delta, target_delta)\n",
    "        \n",
    "        # 2. Position Loss (Auxiliary) - MSE 사용: 절대 위치(x, y) 채점\n",
    "        loss_pos = self.mse(pred_pos, target_pos)\n",
    "        \n",
    "        # 3. Combined: 최종 점수 합산\n",
    "        return loss_delta + self.lambda_pos * loss_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf4de6b-8d62-4ecd-a6bf-4a72a57bf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. 학습 및 실행 로직\n",
    "# ==========================================\n",
    "\n",
    "def train_advanced_model(X, y, game_ids, n_splits=5, epochs=30, batch_size=64):\n",
    "    input_dim = X.shape[2]\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    trained_models = []\n",
    "    \n",
    "    # game_ids에서 _aug 제거 (GroupKFold용)\n",
    "    # augmentation 시 game_id는 유지했으므로 그대로 사용하면 됨\n",
    "    # 1. GroupKFold 교차 검증 준비\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, game_ids)):\n",
    "        print(f\"Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_val, y_val = X[val_idx], y[val_idx]\n",
    "        \n",
    "        # 2. 데이터 로더 설정\n",
    "        train_ds = PassTraceDataset(X_train, y_train)\n",
    "        val_ds = PassTraceDataset(X_val, y_val)\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True) # batch_size(64개)씩/ 학습 데이터 순서 무작위\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)    # 검증 데이터 순서 상관 X \n",
    "\n",
    "        # 3. 모델 및 학습 도구 초기화\n",
    "        model = SoccerLSTM_Advanced(input_dim, HIDDEN_DIM, NUM_LAYERS, DROPOUT, CONTEXT_LEN).to(device)\n",
    "        criterion = CombinedMultiTaskLoss(DELTA, ALPHA, LAMBDA_POS)   # 손실함수 준비\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)             # 옵티마이저 모듈/ 가중치 0.001\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)   # 학습 정체 시 학습률 낮춰 세밀하게 학습\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        patience_cnt = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 4. 학습 루프 *중요*\n",
    "            model.train() # 학습 모드 전환 (Dropout 켜짐)\n",
    "            train_loss = 0\n",
    "            for bx, by in train_loader:\n",
    "                bx, by = bx.to(device), by.to(device)\n",
    "                # 정답 데이터(y)\n",
    "                target_delta = by[:, :2] # dx, dy (이동 벡터)\n",
    "                target_pos = by[:, 2:]   # x, y (절대 위치)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred_delta, pred_pos = model(bx)  # 모델 예측\n",
    "\n",
    "                # 멀티태스크 손실 계산\n",
    "                loss = criterion(pred_delta, target_delta, pred_pos, target_pos)\n",
    "                loss.backward()     # 기울기 계산\n",
    "                optimizer.step()    # 가중치 업데이트\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "            # 5.검증 루프 Validation\n",
    "            model.eval()    # 평가 모드 전환 (Dropout 꺼짐)\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():  # 기울기 계산 끔 (메모리 절약)\n",
    "                for bx, by in val_loader:\n",
    "                    bx, by = bx.to(device), by.to(device)\n",
    "                    target_delta = by[:, :2]\n",
    "                    target_pos = by[:, 2:]\n",
    "                    \n",
    "                    p_delta, p_pos = model(bx)\n",
    "                    loss = criterion(p_delta, target_delta, p_pos, target_pos)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            # 6. 조기 종료 (Earlry Stopping) 및 모델 저장\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                best_state = copy.deepcopy(model.state_dict())  # 최고 점수 시점 저장\n",
    "                patience_cnt = 0\n",
    "                print(f\"  Ep {epoch+1}: Val Loss {avg_val_loss:.4f} (Best)\")\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "                if patience_cnt >= PATIENCE:\n",
    "                    print(\"Early Stopping\")\n",
    "                    break\n",
    "        \n",
    "        model.load_state_dict(best_state)\n",
    "        trained_models.append(model)\n",
    "        \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f53a009-06b3-48e7-b99e-19dbd8e29069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7. 테스트 데이터 준비 (Test Data Preparation)\n",
    "# ==========================================\n",
    "\n",
    "def prepare_test_data_advanced(test_csv_path, match_csv_path, meta, feat_cols, max_len=50):\n",
    "    print(f\"테스트 데이터 인덱스 로드 중: {test_csv_path}\")\n",
    "    df_test_index = pd.read_csv(test_csv_path) # 인덱스 파일 로드\n",
    "    \n",
    "    # 1. Match Info 병합 (Venue, Month 정보 등) - 인덱스 데이터프레임에 병합\n",
    "    if os.path.exists(match_csv_path):\n",
    "        # load_match_info 함수를 재사용하거나 직접 로드\n",
    "        # 여기서는 로직 명확성을 위해 직접 로드 및 처리\n",
    "        df_match = pd.read_csv(match_csv_path)\n",
    "        \n",
    "        # Meta 정보 활용 (Venue ID)\n",
    "        venue2id = meta.get('venue2id', {})\n",
    "        df_match['venue_id'] = df_match['venue'].map(venue2id).fillna(0).astype(int)\n",
    "        \n",
    "        # 날짜 파싱 및 Cyclical Encoding (Train과 동일하게)\n",
    "        if 'game_date' in df_match.columns:\n",
    "            df_match['game_date'] = pd.to_datetime(df_match['game_date'])\n",
    "            df_match['month'] = df_match['game_date'].dt.month\n",
    "            df_match['month_sin'] = np.sin(2 * np.pi * (df_match['month'] - 1) / 12)\n",
    "            df_match['month_cos'] = np.cos(2 * np.pi * (df_match['month'] - 1) / 12)\n",
    "        \n",
    "        # 필요한 컬럼 준비 (venue_id, month_sin, month_cos가 있는지 확인)\n",
    "        cols_to_merge = ['game_id', 'venue_id']\n",
    "        if 'month_sin' in df_match.columns:\n",
    "            cols_to_merge.extend(['month_sin', 'month_cos'])\n",
    "            \n",
    "        # 병합\n",
    "        df_test_index = pd.merge(df_test_index, df_match[cols_to_merge], on='game_id', how='left')\n",
    "        \n",
    "        # 결측치 채우기\n",
    "        for col in ['venue_id', 'month_sin', 'month_cos']:\n",
    "            if col in df_test_index.columns:\n",
    "                df_test_index[col] = df_test_index[col].fillna(0)\n",
    "                if col == 'venue_id':\n",
    "                    df_test_index[col] = df_test_index[col].astype(int)\n",
    "    else:\n",
    "        print(\"Warning: match_info.csv not found for test. Filling meta features with 0.\")\n",
    "        df_test_index['venue_id'] = 0\n",
    "        df_test_index['month_sin'] = 0\n",
    "        df_test_index['month_cos'] = 0\n",
    "\n",
    "    # 3. 데이터 컨테이너\n",
    "    X_list = []\n",
    "    episode_ids = []\n",
    "    last_start_x_n_list = []\n",
    "    last_start_y_n_list = []\n",
    "\n",
    "    print(f\"테스트 데이터 개별 파일 로드 및 처리 중... (총 {len(df_test_index)}개)\")\n",
    "    \n",
    "    # test.csv의 각 행(에피소드)을 순회\n",
    "    for idx, row in df_test_index.iterrows():\n",
    "        episode_id = row['game_episode']\n",
    "        file_path = row['path']\n",
    "        \n",
    "        # [중요] 개별 에피소드 CSV 파일 로드\n",
    "        try:\n",
    "            df_ep = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # [중요] 개별 파일 내부 정렬\n",
    "        df_ep = df_ep.sort_values(['time_seconds', 'action_id']).reset_index(drop=True)\n",
    "        \n",
    "        # Match Info(Venue, Month) 정보를 에피소드 데이터에 붙여넣기\n",
    "        if 'venue_id' in row:\n",
    "            df_ep['venue_id'] = row['venue_id']\n",
    "        if 'month_sin' in row:\n",
    "            df_ep['month_sin'] = row['month_sin']\n",
    "            df_ep['month_cos'] = row['month_cos']\n",
    "        \n",
    "        # 피처 엔지니어링 (is_train=False)\n",
    "        df_ep_feat, _ = make_event_features(df_ep, meta=meta, is_train=False)\n",
    "        \n",
    "        # 시퀀스 구축\n",
    "        # 마지막 패스 찾기\n",
    "        pass_rows = df_ep_feat[df_ep_feat['type_name'] == 'Pass']\n",
    "        if len(pass_rows) == 0:\n",
    "            continue\n",
    "            \n",
    "        last_pass_idx = pass_rows.index[-1]\n",
    "        last_pass = df_ep_feat.loc[last_pass_idx]\n",
    "        \n",
    "        last_x = last_pass['start_x_n']\n",
    "        last_y = last_pass['start_y_n']\n",
    "        \n",
    "        # 입력 시퀀스: 마지막 패스 '이전' 까지\n",
    "        df_input = df_ep_feat[df_ep_feat.index < last_pass_idx].copy()\n",
    "        \n",
    "        if len(df_input) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Feature Selection & Padding\n",
    "        # feat_cols에 없는 컬럼이 있으면 0으로 채움 (안전장치)\n",
    "        for col in feat_cols:\n",
    "            if col not in df_input.columns:\n",
    "                df_input[col] = 0\n",
    "                \n",
    "        seq = df_input[feat_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        seq_vals = seq.to_numpy(dtype=float)\n",
    "        \n",
    "        seq_len = len(seq_vals)\n",
    "        if seq_len > max_len:\n",
    "            seq_vals = seq_vals[-max_len:]\n",
    "        elif seq_len < max_len:\n",
    "            pad = np.zeros((max_len - seq_len, seq_vals.shape[1]), dtype=float)\n",
    "            seq_vals = np.concatenate([pad, seq_vals], axis=0)\n",
    "            \n",
    "        X_list.append(seq_vals)\n",
    "        episode_ids.append(episode_id)\n",
    "        last_start_x_n_list.append(last_x)\n",
    "        last_start_y_n_list.append(last_y)\n",
    "\n",
    "    X_test = np.stack(X_list, axis=0) if X_list else np.array([])\n",
    "    last_start_x_n_arr = np.array(last_start_x_n_list)\n",
    "    last_start_y_n_arr = np.array(last_start_y_n_list)\n",
    "    \n",
    "    return X_test, episode_ids, last_start_x_n_arr, last_start_y_n_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8bc9bc6-3d20-40e3-a199-3b142d92ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8. 예측 및 제출 (Prediction & Submission)\n",
    "# ==========================================\n",
    "\n",
    "def predict_and_submit_advanced(trained_models, X_test, episode_ids, last_x_arr, last_y_arr, submission_path='submission.csv'):\n",
    "    # Test Dataset & Loader\n",
    "    test_ds = PassTraceDataset(X_test) # y=None\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"\\n모델 예측 수행 중 (Ensemble Size: {len(trained_models)})...\")\n",
    "    \n",
    "    all_fold_deltas = []\n",
    "    \n",
    "    # Fold별 추론\n",
    "    for i, model in enumerate(trained_models):\n",
    "        model.eval()\n",
    "        fold_deltas = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for bx in test_loader:\n",
    "                bx = bx.to(device)\n",
    "                \n",
    "                # 모델 출력: (delta, pos)\n",
    "                pred_delta, pred_pos = model(bx)\n",
    "                \n",
    "                # CPU로 이동\n",
    "                fold_deltas.append(pred_delta.cpu().numpy())\n",
    "                \n",
    "        all_fold_deltas.append(np.concatenate(fold_deltas, axis=0))\n",
    "    \n",
    "    # 앙상블 (평균)\n",
    "    avg_deltas = np.mean(all_fold_deltas, axis=0) # (N, 2) -> dx_n, dy_n\n",
    "    \n",
    "    # 좌표 복원 (Denormalization)\n",
    "    pred_dx_n = avg_deltas[:, 0]\n",
    "    pred_dy_n = avg_deltas[:, 1]\n",
    "    \n",
    "    # 최종 좌표 계산: 시작점 + 변화량\n",
    "    pred_end_x_n = last_x_arr + pred_dx_n\n",
    "    pred_end_y_n = last_y_arr + pred_dy_n\n",
    "    \n",
    "    # 실제 스케일(m)로 변환\n",
    "    pred_end_x = pred_end_x_n * PITCH_X\n",
    "    pred_end_y = pred_end_y_n * PITCH_Y\n",
    "    \n",
    "    # 경기장 범위 밖으로 나가는 값 Clip (Safety Guard)\n",
    "    pred_end_x = np.clip(pred_end_x, 0, PITCH_X)\n",
    "    pred_end_y = np.clip(pred_end_y, 0, PITCH_Y)\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'game_episode': episode_ids,\n",
    "        'end_x': pred_end_x,\n",
    "        'end_y': pred_end_y\n",
    "    })\n",
    "    \n",
    "    # 저장\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"\\n✓ 예측 완료 및 저장됨: {submission_path}\")\n",
    "    print(submission.head())\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c53214ec-442c-4d51-939e-3a34256f095b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data Loading & Augmentation...\n",
      "데이터 증강 수행 중 (Flip Y)...\n",
      "Step 2: Feature Engineering...\n",
      "Step 3: Sequence Building...\n",
      "학습 데이터 크기: (30856, 50, 23)\n",
      "Step 4: Training...\n",
      "Fold 1/5\n",
      "  Ep 1: Val Loss 0.2422 (Best)\n",
      "  Ep 2: Val Loss 0.2276 (Best)\n",
      "  Ep 3: Val Loss 0.2232 (Best)\n",
      "  Ep 4: Val Loss 0.2185 (Best)\n",
      "  Ep 5: Val Loss 0.2147 (Best)\n",
      "  Ep 6: Val Loss 0.2044 (Best)\n",
      "  Ep 7: Val Loss 0.1986 (Best)\n",
      "  Ep 9: Val Loss 0.1963 (Best)\n",
      "  Ep 10: Val Loss 0.1934 (Best)\n",
      "  Ep 11: Val Loss 0.1918 (Best)\n",
      "  Ep 12: Val Loss 0.1912 (Best)\n",
      "  Ep 13: Val Loss 0.1894 (Best)\n",
      "  Ep 18: Val Loss 0.1880 (Best)\n",
      "Early Stopping\n",
      "Fold 2/5\n",
      "  Ep 1: Val Loss 0.2406 (Best)\n",
      "  Ep 2: Val Loss 0.2263 (Best)\n",
      "  Ep 3: Val Loss 0.2216 (Best)\n",
      "  Ep 4: Val Loss 0.2193 (Best)\n",
      "  Ep 5: Val Loss 0.2128 (Best)\n",
      "  Ep 6: Val Loss 0.2048 (Best)\n",
      "  Ep 7: Val Loss 0.2003 (Best)\n",
      "  Ep 8: Val Loss 0.1960 (Best)\n",
      "  Ep 9: Val Loss 0.1960 (Best)\n",
      "  Ep 10: Val Loss 0.1912 (Best)\n",
      "  Ep 11: Val Loss 0.1893 (Best)\n",
      "  Ep 12: Val Loss 0.1876 (Best)\n",
      "  Ep 14: Val Loss 0.1867 (Best)\n",
      "  Ep 15: Val Loss 0.1861 (Best)\n",
      "  Ep 16: Val Loss 0.1853 (Best)\n",
      "  Ep 21: Val Loss 0.1848 (Best)\n",
      "Early Stopping\n",
      "Fold 3/5\n",
      "  Ep 1: Val Loss 0.2413 (Best)\n",
      "  Ep 2: Val Loss 0.2228 (Best)\n",
      "  Ep 3: Val Loss 0.2165 (Best)\n",
      "  Ep 4: Val Loss 0.2128 (Best)\n",
      "  Ep 6: Val Loss 0.2077 (Best)\n",
      "  Ep 7: Val Loss 0.1930 (Best)\n",
      "  Ep 8: Val Loss 0.1911 (Best)\n",
      "  Ep 9: Val Loss 0.1877 (Best)\n",
      "  Ep 11: Val Loss 0.1848 (Best)\n",
      "  Ep 13: Val Loss 0.1827 (Best)\n",
      "  Ep 18: Val Loss 0.1817 (Best)\n",
      "Early Stopping\n",
      "Fold 4/5\n",
      "  Ep 1: Val Loss 0.2373 (Best)\n",
      "  Ep 2: Val Loss 0.2271 (Best)\n",
      "  Ep 3: Val Loss 0.2186 (Best)\n",
      "  Ep 4: Val Loss 0.2152 (Best)\n",
      "  Ep 5: Val Loss 0.2086 (Best)\n",
      "  Ep 6: Val Loss 0.2006 (Best)\n",
      "  Ep 7: Val Loss 0.1953 (Best)\n",
      "  Ep 8: Val Loss 0.1951 (Best)\n",
      "  Ep 9: Val Loss 0.1936 (Best)\n",
      "  Ep 10: Val Loss 0.1896 (Best)\n",
      "  Ep 11: Val Loss 0.1873 (Best)\n",
      "  Ep 12: Val Loss 0.1857 (Best)\n",
      "  Ep 15: Val Loss 0.1849 (Best)\n",
      "Early Stopping\n",
      "Fold 5/5\n",
      "  Ep 1: Val Loss 0.2392 (Best)\n",
      "  Ep 2: Val Loss 0.2259 (Best)\n",
      "  Ep 3: Val Loss 0.2209 (Best)\n",
      "  Ep 4: Val Loss 0.2155 (Best)\n",
      "  Ep 5: Val Loss 0.2093 (Best)\n",
      "  Ep 6: Val Loss 0.2064 (Best)\n",
      "  Ep 7: Val Loss 0.2003 (Best)\n",
      "  Ep 8: Val Loss 0.1956 (Best)\n",
      "  Ep 9: Val Loss 0.1946 (Best)\n",
      "  Ep 10: Val Loss 0.1931 (Best)\n",
      "  Ep 11: Val Loss 0.1929 (Best)\n",
      "  Ep 12: Val Loss 0.1901 (Best)\n",
      "  Ep 13: Val Loss 0.1891 (Best)\n",
      "  Ep 18: Val Loss 0.1861 (Best)\n",
      "Early Stopping\n",
      "\n",
      "[Step 5] Predicting on Test Data...\n",
      "테스트 데이터 인덱스 로드 중: test.csv\n",
      "테스트 데이터 개별 파일 로드 및 처리 중... (총 2414개)\n",
      "Test Set Shape: (2414, 50, 23)\n",
      "\n",
      "모델 예측 수행 중 (Ensemble Size: 5)...\n",
      "\n",
      "✓ 예측 완료 및 저장됨: submission_advanced.csv\n",
      "  game_episode      end_x      end_y\n",
      "0     153363_1  60.284557  10.508844\n",
      "1     153363_2  27.408899  47.849925\n",
      "2     153363_6  35.739226  62.805128\n",
      "3     153363_7  52.402287   8.120887\n",
      "4     153363_8  80.757282   5.077972\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 실행 (Main)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 데이터 로드 및 전처리\n",
    "    print(\"1. 데이터 로드 및 전처리...\")\n",
    "    df_train, match_meta = load_raw_and_process('train.csv', 'match_info.csv', is_train=True)\n",
    "    \n",
    "    # 2. 피처 엔지니어링\n",
    "    print(\"2. 피처 엔지니어링...\")\n",
    "    df_train_feat, meta = make_event_features(df_train, meta=match_meta, is_train=True)\n",
    "    \n",
    "    # 3. 시퀀스 구축\n",
    "    print(\"3. 시퀀스 구축...\")\n",
    "    X, y, info_df, feat_cols = build_episode_sequences(df_train_feat, meta)\n",
    "    \n",
    "    print(f\"학습 데이터 크기: {X.shape}\") \n",
    "    \n",
    "    # 4. 학습 및 실행 로직\n",
    "    print(\"Step 4: Training...\")\n",
    "    models = train_advanced_model(X, y, info_df['game_id'].values)\n",
    "\n",
    "    # 5. 테스트 데이터 준비\n",
    "    print(\"\\n[Step 5] Predicting on Test Data...\")\n",
    "    \n",
    "    X_test, test_ids, last_x_test, last_y_test = prepare_test_data_advanced(\n",
    "        test_csv_path='test.csv',\n",
    "        match_csv_path='match_info.csv',\n",
    "        meta=meta,           # 학습 시 생성된 메타데이터\n",
    "        feat_cols=feat_cols, # venue_id가 포함된 피처 리스트\n",
    "        max_len=MAX_SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Set Shape: {X_test.shape}\")\n",
    "    \n",
    "    # 6. 예측 및 제출\n",
    "    if len(models) > 0:\n",
    "        submission = predict_and_submit_advanced(\n",
    "            trained_models=models,\n",
    "            X_test=X_test,\n",
    "            episode_ids=test_ids,\n",
    "            last_x_arr=last_x_test,\n",
    "            last_y_arr=last_y_test,\n",
    "            submission_path='submission_advanced.csv'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Error: 학습된 모델이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f4db2-61f6-48c1-aa38-fb0bcda6da11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
